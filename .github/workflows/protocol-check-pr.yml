# workflows/protocol-check-pr.yml
#
# Protocol: Check PR
# Checks a protocol PR for building, linting, and clang standards success

name: "Protocol: Check PR"

on:
  push:
    # Trigger runs on our default branch, `dev`, to upload code coverage reports to Codecov
    branches:
      - dev
    paths:
      - "protocol/**"
      - "!protocol/**/README.md"
      - ".github/workflows/protocol-check-pr.yml"
      - "mandelboxes/helper_scripts/copy_protocol_build.sh"
  pull_request:
    paths:
      - "protocol/**"
      - "!protocol/**/README.md"
      - ".github/workflows/protocol-check-pr.yml"
      - "mandelboxes/helper_scripts/copy_protocol_build.sh"
  workflow_dispatch:

jobs:
  # Check Protocol PR on Intel x86_64 systems (via GHA runners)
  protocol-check-pr-x64:
    name: ${{ matrix.config.name }}
    runs-on: ${{ matrix.config.os }}

    strategy:
      matrix:
        config:
          - name: "Build and Check Protocol on Windows (cl)"
            os: windows-2019 # llvm, cmake, clang-tidy are pre-installed on windows-2019+ GHA machines
            cuda: false # Set to true to CUDA-optimize the windows server executable. Note that CUDA takes 5-7min to install.
          - name: "Build and Check Protocol on Ubuntu Linux (gcc)"
            os: ubuntu-20.04 # cmake is pre-installed on ubuntu-20.04+ machines
          - name: "Build and Check Protocol on macOS x64 (clang)"
            os: macos-10.15 # cmake is pre-installed on macos-10.15+ machines

    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

    steps:
      ################################## CONFIG STEPS START ##############################

      - name: Checkout Git Repository
        uses: actions/checkout@v2

      # Necessary for downloading protocol libs and base Dockerfile NVIDIA GRID drivers from AWS S3
      - name: Configure AWS S3 CLI
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_GITHUB_ACTIONS_USER_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_GITHUB_ACTIONS_USER_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      ###################################################################################
      ################################ WINDOWS STEPS START ##############################

      - name: Install Chocolatey dependencies on Windows GHA Machine
        if: runner.os == 'Windows'
        uses: crazy-max/ghaction-chocolatey@v1
        with:
          args: install cppcheck ninja

      - name: Set up Visual Studio Developer Command Prompt (for nmake) on Windows GHA Machine
        if: runner.os == 'Windows'
        uses: ilammy/msvc-dev-cmd@v1

      - name: Install CUDA on Windows GHA Machine
        if: runner.os == 'Windows' && matrix.config.cuda
        working-directory: .github\workflows\helpers\protocol
        env:
          visual_studio: "Visual Studio 16 2019" # Imported from env in the .ps1 script
          cuda: "11.0.167" # Imported from env in the .ps1 script, we use CUDA 11.0
        shell: powershell
        run: |
          # Install CUDA and set paths
          .\install_cuda_windows.ps1
          if ($?) {
            # Set paths for subsequent steps, using $env:CUDA_PATH
            echo "Adding CUDA to CUDA_PATH, CUDA_PATH_X_Y and PATH"
            echo "CUDA_PATH=$env:CUDA_PATH" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
            echo "$env:CUDA_PATH_VX_Y=$env:CUDA_PATH" | Out-File -FilePath $env:GITHUB_ENV -Encoding utf8 -Append
            echo "$env:CUDA_PATH/bin" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
          } else {
            exit 1
          }

      - name: Verify that NVCC is Installed on GHA Windows Machine
        if: runner.os == 'Windows' && matrix.config.cuda
        shell: powershell
        run: |
          nvcc -V
          ls $env:CUDA_PATH
          ls $env:CUDA_PATH\bin
          ls $env:CUDA_PATH\include

      - name: Register OS Compiler Matchers
        shell: bash --noprofile --norc -eo pipefail {0}
        run: |
          # OS-independent matchers
          echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/cppcheck_matcher.json"
          echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/cmake_matcher.json"

          # OS-specific matchers
          if [ "$RUNNER_OS" == "Linux" ]; then
              echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/gcc_matcher.json"
          elif [ "$RUNNER_OS" == "Windows" ]; then
              echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/msvc_matcher.json"
          elif [ "$RUNNER_OS" == "macOS" ]; then
              echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/clang_matcher.json"
          fi

      - name: Build and Test Protocol Client and Server on Windows
        if: runner.os == 'Windows'
        shell: cmd # Acts as Visual Studio Developer Command Prompt due to ilammy/msvc-dev-cmd@v1
        working-directory: protocol
        run: |
          mkdir build
          cd build
          cmake .. -DCMAKE_BUILD_TYPE=Debug -DCHECK_CI=TRUE -G "Ninja" || EXIT /B 1
          ninja WhistClient || EXIT /B 1
          ninja WhistServer || EXIT /B 1
          ninja test || EXIT /B 1

      - name: Verify builds with clang-tidy on Windows
        if: runner.os == 'Windows'
        shell: cmd # Acts as Visual Studio Developer Command Prompt due to ilammy/msvc-dev-cmd@v1
        working-directory: protocol
        run: |
          REM Clang-matcher for clang-tidy, added after the main build sequence so that clang doesn't match gcc/msvc errors
          echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/clang_matcher.json"

          REM Run clang-tidy
          cd build
          ninja clang-tidy || EXIT /B 1

      ###################################################################################
      ################################# MACOS STEPS START ###############################

      - name: Install macOS Client Protocol Dependencies on macOS GHA Machines
        if: runner.os == 'macOS'
        working-directory: protocol
        run: |
          # llvm for clang-tidy, coreutils for realpath, lcov for codecov
          brew install llvm coreutils perl doxygen lcov
          ln -s "/usr/local/opt/llvm/bin/clang-format" "/usr/local/bin/clang-format"
          ln -s "/usr/local/opt/llvm/bin/clang-tidy" "/usr/local/bin/clang-tidy"

      - name: Build and Test Protocol Client on macOS
        if: runner.os == 'macOS'
        working-directory: protocol
        env:
          WHIST_OSXSYSROOT: "/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk"
        run: |
          # Build the protocol and its test suite
          mkdir build
          cd build
          cmake .. -DCMAKE_BUILD_TYPE=Debug -DCHECK_CI=TRUE
          make -j WhistClient
          make -j test

      - name: Verify Builds with clang-tidy on macOS
        if: runner.os == 'macOS'
        working-directory: protocol
        run: |
          # Clang-matcher for clang-tidy, added after the main build sequence so that clang doesn't match gcc/msvc errors
          echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/clang_matcher.json"

          # Run clang-tidy
          cd build
          make -j clang-tidy

      ###################################################################################
      ################################# LINUX STEPS START ###############################

      - name: Build Protocol Client on Ubuntu Linux
        if: runner.os == 'Linux'
        working-directory: protocol
        run: ./build_protocol_targets.sh --cmakebuildtype=Debug --cmakesetCI --sanitize=address+undefined WhistClient

      - name: Build Protocol Server on Ubuntu Linux
        if: runner.os == 'Linux'
        working-directory: protocol
        run: ./build_protocol_targets.sh --cmakebuildtype=Debug --cmakesetCI --sanitize=address+undefined WhistServer

      - name: Run Protocol Tests on Ubuntu Linux
        if: runner.os == 'Linux'
        working-directory: protocol
        run: ./build_protocol_targets.sh --cmakebuildtype=Debug --cmakesetCI --sanitize=address+undefined test

      # Verify clang-format on Linux
      - name: Verify Builds with clang-format
        if: runner.os == 'Linux'
        working-directory: protocol
        run: |
          # Clang-matcher for clang-format, added after the main build sequence so that clang doesn't match gcc errors
          echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/clang_matcher.json"

          # Print clang-format version for local comparison
          clang-format --version

          # Run clang-format
          ./build_protocol_targets.sh clang-format

          # Show diff in GitHub Actions logs
          git diff HEAD .

          # This will return an error if there is a non-empty diff
          git diff-index --quiet HEAD .

      - name: Verify Builds with clang-tidy on Ubuntu Linux
        if: runner.os == 'Linux'
        working-directory: protocol
        run: |
          # Clang-matcher for clang-tidy, added after the main build sequence so that clang doesn't match gcc/msvc errors
          echo " ::add-matcher::${{ github.workspace }}/.github/workflows/helpers/protocol/clang_matcher.json"

          # Run clang-tidy
          ./build_protocol_targets.sh clang-tidy

      # This tests building the protocol in a container, the same way we do it in production
      - name: Build Protocol Server in Docker Container
        if: runner.os == 'Linux'
        working-directory: protocol
        run: ./build_protocol_targets.sh --cmakebuildtype=Release WhistServer

      # This ensures that all the build files from build_protocol_targets.sh have actually been generated
      - name: Ensure that all protocol build files are present
        if: runner.os == 'Linux'
        working-directory: mandelboxes
        run: ./helper_scripts/copy_protocol_build.sh

      - name: Lint CMake Files
        if: runner.os == 'Linux'
        working-directory: protocol
        run: |
          pip install cmakelint
          sudo apt-get install fd-find
          # Ubuntu-specific PATH fix for `fd`
          ln -s $(which fdfind) ~/.local/bin/fd
          ./lint-cmake.sh

  #######################################################################################
  #######################################################################################

  # Check Protocol PR on ARM64 systems (via self-hosted MacStadium M1 runner)
  protocol-check-pr-arm64:
    name: Build and Test Protocol on macOS ARM64 (clang)
    runs-on: [self-hosted, macOS, ARM64]
    defaults:
      run:
        shell: "/usr/bin/arch -arch arm64e /bin/bash {0}"

    steps:
      ################################## CONFIG STEPS START ##############################

      - name: Checkout Git Repository
        uses: actions/checkout@v2

      # Necessary for downloading protocol libs and base Dockerfile NVIDIA GRID drivers from AWS S3
      - name: Configure AWS S3 CLI
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_GITHUB_ACTIONS_USER_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_GITHUB_ACTIONS_USER_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      ###################################################################################
      ################################# MACOS STEPS START ###############################

      # These commands must be run with sudo, which requires entering the password. They need
      # to be run via VNC directly in the terminal, so that the password can be provided. They
      # are left here, commented out, for reference
      # - name: Install macOS Client Protocol Dependencies
      #   if: runner.os == 'macOS'
      #   working-directory: protocol
      #   run: |
      #     # llvm for clang-tidy, coreutils for realpath, lcov for codecov
      #     brew install llvm coreutils perl doxygen lcov
      #     sudo ln -s "/opt/homebrew/opt/llvm/bin/clang-format" "/usr/local/bin/clang-format"
      #     sudo ln -s "/opt/homebrew/opt/llvm/bin/clang-tidy" "/usr/local/bin/clang-tidy"

      - name: Build and Test Protocol Client
        working-directory: protocol
        env:
          WHIST_OSXSYSROOT: "/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk"
        run: |
          mkdir build
          cd build
          cmake .. -DCMAKE_BUILD_TYPE=Debug -DCHECK_CI=TRUE
          make -j WhistClient
          make -j test

      # Note that we don't run clang-tidy on arm64 macOS, since we already run it on x64 macOS

  #######################################################################################
  #######################################################################################

  # This job runs a protocol streaming session between two Docker containers on the same
  # AWS EC2 instance to test end-to-end networking performance in a controlled environment.
  protocol-streaming-e2e-check-pr:
    name: "Protocol Streaming End-to-End Test"
    runs-on: ubuntu-20.04

    steps:
      - name: Checkout Git Repository
        uses: actions/checkout@v2

      - name: Setup Python-based notifications
        working-directory: .github/workflows/helpers
        run: ./notifications/setup_notifications.sh true

      - name: Configure AWS S3 CLI
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_GITHUB_ACTIONS_USER_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_GITHUB_ACTIONS_USER_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      # Load the AWS SSH keypair onto the runner, so that it can run remote commands on the AWS EC2 machine
      # running the streaming end-to-end test. This keypair is defined in AWS and stored as a GitHub Secret.
      - name: Install SSH key
        uses: shimataro/ssh-key-action@v2
        with:
          name: id_rsa
          key: ${{ secrets.PROTOCOL_E2E_SREAMING_TEST_INSTANCE_SSH_KEY }}
          known_hosts: unnecessary # Note, this is not secure and discouraged, but tolerable here given the instance is deleted after the test
          if_key_exists: fail

      - name: Install End-to-End Streaming Test Dependencies
        run: pip install boto3 paramiko numpy pytablewriter

      # Run the streaming end-to-end integration test. The -ssh-key-name must match the value in AWS EC2 under
      # Key Pairs, where the SSH key is defined. Other possible flags:
      # --testing_url, defaults to Big Buck Bunny video stored on our AWS S3
      # --testing_time, defaults to 126s
      # --cmake-build-type, defaults to `Metrics`
      # --region-name, defaults to `us-east-1`
      # --use-two-instances, defaults to `false`
      # --use-existing-server-instance, defaults to `false`
      # --use-existing-client-instance, defaults to `false`
      # --aws-credentials-filepath, defaults to `~/.aws/credentials`

      ### Action below will run the End-to-End Streaming Test for each new commit to a branch of a PR. ###
      - name: Run End-to-End Streaming Test
        id: e2e_streaming_test
        working-directory: protocol/test
        run: python3 streaming_e2e_tester.py --ssh-key-name "GITHUB_ACTIONS_E2E_PERFORMANCE_TEST_SSH_KEYPAIR" --ssh-key-path "/home/runner/.ssh/id_rsa" --github-token "${{ secrets.GHA_PERSONAL_ACCESS_TOKEN }}" --use-existing-server-instance i-0a6d434cc397b1dc3

      # need one more section like this to ensure the S3 command works in case of failure/timeout of the Python script above
      - name: Upload logs to S3 for debugging purposes
        if: always()
        working-directory: protocol/test
        run: aws s3 cp perf_logs s3://whist-e2e-protocol-test-logs/"$GITHUB_REF_NAME" --recursive

      - name: Stop or terminate leftover AWS instances
        if: ${{ always() && (steps.e2e_streaming_test.outcome == 'failure') }}
        working-directory: protocol/test
        run: python3 e2e_helpers/clean_leftover_instances.py

      # The streaming_e2e_tester.py file generates the files server_monitoring_log.txt and client_monitoring_log.txt in
      # the folder perf_logs at the path where it is run in. We parse these files to display the results of the test.
      - name: Parse & Display Test Results
        working-directory: protocol/test
        shell: python3 {0}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_ISSUE: ${{ github.event.number }}
          GITHUB_PR_URL: "https://github.com/whisthq/whist/pull/"
          SLACK_WEBHOOK: ${{ secrets.SLACK_HOOKS_ENDPOINT_DEV }}
        run: |
          import os, sys, time
          import numpy as np
          from pytablewriter import MarkdownTableWriter
          from contextlib import redirect_stdout

          # Here, we parse the test results into a .info file, which can be read and displayed on the GitHub PR
          # Create output .info file
          results_file = open("streaming_e2e_test_results.info", "w+")

          # Find the path to the folder with the most recent E2E protocol logs
          logs_root_dir = "perf_logs"
          test_time = ""
          for folder_name in os.listdir("./perf_logs"):
            if time.strftime("%Y_%m_%d@") in folder_name:
              logs_root_dir = os.path.join(logs_root_dir, folder_name)
              test_time = folder_name
              break
          if logs_root_dir == "perf_logs":
            print("Error: protocol logs not found!")
            exit(-1)


          # Check if the log files with metrics are present
          client_log_file = os.path.join(logs_root_dir, "client", "client.log")
          server_log_file = os.path.join(logs_root_dir, "server", "server.log")

          if not os.path.isfile(client_log_file):
            print("Error, client log file {} does not exist".format(client_log_file))
            exit(-1)
          if not os.path.isfile(server_log_file):
            print("Error, server log file {} does not exist".format(server_log_file))
            exit(-1)

          # Extract the metric values and save them in two dictionaries
          client_metrics = {}
          server_metrics = {}

          with open(client_log_file, "r") as f:
            for line in f.readlines():
              if line[0] == "{":
                l = line.strip().split()
                metric_name = l[1].strip("\"")
                metric_value = float(l[3].strip("\""))
                if metric_name not in client_metrics:
                  client_metrics[metric_name] = [metric_value]
                else:
                  client_metrics[metric_name].append(metric_value)

          with open(server_log_file, "r") as f:
            for line in f.readlines():
              if line[0] == "{":
                l = line.strip().split()
                metric_name = l[1].strip("\"")
                metric_value = float(l[3].strip("\""))
                if metric_name not in server_metrics:
                  server_metrics[metric_name] = [metric_value]
                else:
                  server_metrics[metric_name].append(metric_value)

          client_metrics2 = []
          server_metrics2 = []

          for k in client_metrics:
            client_metrics[k] = np.array(client_metrics[k])
            client_metrics2.append({
              'metric': k,
              'entries': len(client_metrics[k]),
              'avg': np.mean(client_metrics[k]), 
              'std': np.std(client_metrics[k]), 
              'max': np.max(client_metrics[k]), 
              'min':np.min(client_metrics[k])
            })

          for k in server_metrics:
            server_metrics[k] = np.array(server_metrics[k])
            server_metrics2.append({
              'metric': k,
              'entries': len(server_metrics[k]),
              'avg': np.mean(server_metrics[k]), 
              'std': np.std(server_metrics[k]), 
              'max': np.max(server_metrics[k]), 
              'min':np.min(server_metrics[k])
            })


          with redirect_stdout(results_file):

            if len(client_metrics) == 0:
              print("NO CLIENT METRICS\n")
            else:
              print("###### CLIENT METRICS: ######\n")

            writer = MarkdownTableWriter(
              #table_name="Client metrics",
              headers=["Metric", "Entries", "Average", "Standard Deviation", "Min", "Max"],
              value_matrix=[
                [i['metric'], i['entries'], i['avg'], i['std'], i['min'], i['max']] for i in client_metrics2
              ],
              margin=1,  # add a whitespace for both sides of each cell
              max_precision=4,
            )
            writer.write_table()
            print("\n")

            if len(server_metrics) == 0:
              print("NO SERVER METRICS\n")
            else:
              print("###### SERVER METRICS: ######\n")


            writer = MarkdownTableWriter(
              #table_name="Client metrics",
              headers=["Metric", "Entries", "Average", "Standard Deviation", "Min", "Max"],
              value_matrix=[
                [i['metric'], i['entries'], i['avg'], i['std'], i['min'], i['max']] for i in server_metrics2
              ],
              margin=1,  # add a whitespace for both sides of each cell
              max_precision=4,
            )
            writer.write_table()


          results_file.close()

          #######################################################################################

          # Display the results on the GitHub PR. In the future, we could also upload summaries to Logz.io, or post
          # a nightly update in Slack, if we choose to.

          sys.path.append(".github/workflows/helpers")
          from notifications.github_bot import github_comment_update

          if not os.environ.get("GITHUB_REF_NAME"):
            print("No GitHub PR/Issue number! Skipping PR notification.")
            sys.exit(0)

          github_token = os.environ["GITHUB_TOKEN"]
          github_issue = int(os.environ["GITHUB_REF_NAME"].split("/")[0])
          github_repo = "whisthq/whist"

          identifier = "AUTOMATED_STREAMING_E2E_TEST_RESULTS_MESSAGE"
          title = "Protocol End-to-End Streaming Test Results - {}".format(test_time)

          # This file gets created above, in parsing the test results into something more human-readable
          f = open("streaming_e2e_test_results.info", "r")
          body = f.read()
          f.close()

          github_comment_update(
            github_token,
            github_repo,
            github_issue,
            identifier,
            body,
            title=title,
          )

          # Post updates to Slack channel
          from notifications.slack_bot import slack_post

          if not os.environ.get("SLACK_WEBHOOK"):
            print("Error, cannot post updates to Slack because SLACK_WEBHOOK was not set")
            exit(-1)
          slack_webhook = os.environ.get("SLACK_WEBHOOK")

          slack_post(slack_webhook, body=body, slack_username="Whist Bot", title=title)

  #######################################################################################
